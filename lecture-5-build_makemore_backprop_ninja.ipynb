{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "## makemore: becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sFElPqq8PPp"
   },
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import typing as t\n",
    "import itertools as it\n",
    "import collections as c\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import time\n",
    "import functools as ft\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import heapq\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.utils.tensorboard as tb\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "plt.rcParams[\"font.size\"] = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V_zt2QHr8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eg20-vsg8PPt"
   },
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MJPU8HT08PPu"
   },
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 35004, 156252,  94096,  37800,  24686,   2101, 153392,  51800, 119439,\n",
       "        115216,  52315,  95635,  28832, 175900, 131259, 166755,  26138, 141888,\n",
       "         43989,   1857,    350, 130503,  28597,  20657, 177075,  67700, 127749,\n",
       "         67638,  14772,  35990,  20280, 115911])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(low=0, high=Xtr.shape[0]+1, size=(batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "8ofj1s6d8PPv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4192, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 26,  5,  9, 25,  1,  0,  0,  0,  8,  1, 20,  5,  9, 12,  0,  0,  0,\n",
       "         0,  0, 11,  0,  0,  5,  9, 19, 20,  0,  0, 12,  1, 21, 14, 24, 12,  5,\n",
       "         9, 11,  5,  0,  0,  0,  0, 11,  1,  1, 11, 25,  0,  0, 19,  1, 14,  9,\n",
       "         0, 11,  1,  4,  1, 10,  0, 11, 14,  1,  1, 14, 19,  8, 15,  0,  0, 18,\n",
       "        12,  5,  5, 15, 14,  9,  0, 26,  9,  0,  0, 13,  0,  0, 11,  0,  0,  0,\n",
       "         1, 26,  9,  0,  0,  0])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 96])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demb.view(-1,demb.shape[-1]).T.unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of embedding matrix dC vectorised\n",
      "C               | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "Gradient of embedding matrix dC Iterative\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Vectorised solution to compute gradients of embedding matrix C.\n",
    "\n",
    "Creating 3D matrics\n",
    "- Unroll the context words with Xb.view(-1) [1 x 96] and unroll demb.view(-1, 10) [96 x 10]\n",
    "- We convert Xb into a OHE of dim [27 x 1 x 96] (after appropriate transposing and adding dim)\n",
    "- We convert demb into a 3D tensor of dim [1 x 10 x 96].\n",
    "\n",
    "Now when we perform the multiplication of these matrices we will produce a 3D tensor of shape\n",
    "[27 x 10 x 96]. Such that across the third dimension of training examples we will have exactly\n",
    "one nonzero row corresponding to the index of the embedding matrix to whom the gradient should flow.\n",
    "We sum across the third dimension to produce the final gradient matrix.\n",
    "\"\"\"\n",
    "# 3D Solution\n",
    "demb_flat = demb.view(-1,demb.shape[-1]).T.unsqueeze(dim=0)  # 1 x 10 x 96 => 10 dim of embed\n",
    "oh = F.one_hot(Xb.view(-1), num_classes=vocab_size).T.unsqueeze(dim=1)  # 27 x 1 x 96; 96 => 3 ctx words x 32 batch sz\n",
    "dC3D = (oh * demb_flat)  # 27 x 10 x 96\n",
    "dC_vec = dC3D.sum(dim=2)\n",
    "print(\"Gradient of embedding matrix dC vectorised\")\n",
    "cmp('C', dC_vec, C)\n",
    "\n",
    "# Compare with iterative solution\n",
    "dC_it = torch.zeros_like(C)\n",
    "for cix, grad in zip(Xb.view(-1), demb.view(-1,demb.shape[-1])):\n",
    "  dC_it[cix] += grad\n",
    "print(\"Gradient of embedding matrix dC Iterative\")  \n",
    "cmp('C', dC_it, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0048,  0.0022,  0.0148,  0.0255,  0.0097,  0.0018,  0.0057,  0.0065,\n",
       "         -0.0103, -0.0113]),\n",
       " tensor([ 0.0048,  0.0022,  0.0148,  0.0255,  0.0097,  0.0018,  0.0057,  0.0065,\n",
       "         -0.0103, -0.0113]))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC_vec[0], dC_it[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0048, 0.0048), (0.0022, 0.0022), (0.0148, 0.0148), (0.0255, 0.0255), (0.0097, 0.0097), (0.0018, 0.0018), (0.0057, 0.0057), (0.0065, 0.0065), (-0.0103, -0.0103), (-0.0113, -0.0113)]\n",
      "[(0.0134, 0.0134), (0.0128, 0.0128), (-0.0066, -0.0066), (0.0089, 0.0089), (-0.0039, -0.0039), (-0.0144, -0.0144), (-0.0073, -0.0073), (-0.0062, -0.0062), (-0.0017, -0.0017), (0.0153, 0.0153)]\n",
      "[(0.0049, 0.0049), (-0.001, -0.001), (-0.0002, -0.0002), (-0.0021, -0.0021), (-0.0034, -0.0034), (-0.0043, -0.0043), (0.0035, 0.0035), (0.002, 0.002), (0.0028, 0.0028), (-0.0005, -0.0005)]\n",
      "[(-0.0092, -0.0092), (-0.0012, -0.0012), (0.0042, 0.0042), (-0.0043, -0.0043), (-0.0029, -0.0029), (0.0015, 0.0015), (-0.0098, -0.0098), (-0.0008, -0.0008), (-0.0085, -0.0085), (0.0071, 0.0071)]\n",
      "[(-0.0094, -0.0094), (-0.0031, -0.0031), (0.0012, 0.0012), (-0.0068, -0.0068), (0.0081, 0.0081), (0.001, 0.001), (-0.0036, -0.0036), (0.0084, 0.0084), (0.0028, 0.0028), (-0.0003, -0.0003)]\n",
      "[(0.0157, 0.0157), (0.0008, 0.0008), (-0.0132, -0.0132), (-0.0155, -0.0155), (-0.0104, -0.0104), (0.01, 0.01), (0.0146, 0.0146), (-0.0008, -0.0008), (0.0057, 0.0057), (0.0087, 0.0087)]\n",
      "[(-0.0036, -0.0036), (-0.0006, -0.0006), (0.0008, 0.0008), (0.0006, 0.0006), (-0.0019, -0.0019), (0.0008, 0.0008), (0.0055, 0.0055), (0.003, 0.003), (0.0001, 0.0001), (-0.0013, -0.0013)]\n",
      "[(0.0001, 0.0001), (0.0145, 0.0145), (-0.0016, -0.0016), (-0.0021, -0.0021), (-0.0144, -0.0144), (0.01, 0.01), (-0.0027, -0.0027), (0.0082, 0.0082), (-0.0038, -0.0038), (-0.0064, -0.0064)]\n",
      "[(-0.0006, -0.0006), (-0.01, -0.01), (0.0024, 0.0024), (-0.0038, -0.0038), (-0.0005, -0.0005), (0.0022, 0.0022), (0.0137, 0.0137), (-0.0209, -0.0209), (-0.0007, -0.0007), (0.0004, 0.0004)]\n",
      "[(-0.0021, -0.0021), (-0.0022, -0.0022), (0.0014, 0.0014), (0.0021, 0.0021), (-0.002, -0.002), (0.0011, 0.0011), (0.0017, 0.0017), (0.0024, 0.0024), (0.0024, 0.0024), (0.0007, 0.0007)]\n",
      "[(-0.003, -0.003), (-0.008, -0.008), (0.0124, 0.0124), (-0.0029, -0.0029), (0.0277, 0.0277), (0.0019, 0.0019), (-0.0127, -0.0127), (-0.0088, -0.0088), (-0.0023, -0.0023), (-0.012, -0.012)]\n",
      "[(0.0056, 0.0056), (0.0065, 0.0065), (0.0045, 0.0045), (-0.0032, -0.0032), (-0.0068, -0.0068), (-0.0081, -0.0081), (-0.001, -0.001), (-0.004, -0.004), (0.003, 0.003), (0.0017, 0.0017)]\n",
      "[(-0.0008, -0.0008), (0.0, 0.0), (-0.0015, -0.0015), (-0.0024, -0.0024), (0.0029, 0.0029), (-0.0023, -0.0023), (0.003, 0.003), (-0.0083, -0.0083), (0.0009, 0.0009), (-0.0036, -0.0036)]\n",
      "[(0.0015, 0.0015), (-0.0185, -0.0185), (0.0014, 0.0014), (0.0053, 0.0053), (0.0037, 0.0037), (-0.0035, -0.0035), (0.0038, 0.0038), (-0.0066, -0.0066), (0.0087, 0.0087), (0.0005, 0.0005)]\n",
      "[(0.0022, 0.0022), (-0.0006, -0.0006), (-0.0044, -0.0044), (-0.0079, -0.0079), (-0.0045, -0.0045), (-0.0007, -0.0007), (-0.0031, -0.0031), (-0.0059, -0.0059), (0.0023, 0.0023), (-0.0008, -0.0008)]\n",
      "[(0.0039, 0.0039), (0.0069, 0.0069), (0.0015, 0.0015), (0.0046, 0.0046), (0.002, 0.002), (-0.0003, -0.0003), (0.0024, 0.0024), (0.0062, 0.0062), (0.0002, 0.0002), (0.0033, 0.0033)]\n",
      "[(-0.0069, -0.0069), (0.0038, 0.0038), (-0.0054, -0.0054), (-0.0055, -0.0055), (0.0005, 0.0005), (0.0045, 0.0045), (0.0016, 0.0016), (0.0114, 0.0114), (0.0015, 0.0015), (-0.0006, -0.0006)]\n",
      "[(-0.0073, -0.0073), (0.0017, 0.0017), (-0.0078, -0.0078), (-0.0002, -0.0002), (0.0011, 0.0011), (-0.0002, -0.0002), (0.0005, 0.0005), (0.0082, 0.0082), (0.004, 0.004), (-0.0044, -0.0044)]\n",
      "[(-0.0092, -0.0092), (-0.0039, -0.0039), (-0.0038, -0.0038), (0.0096, 0.0096), (-0.0049, -0.0049), (-0.0007, -0.0007), (-0.016, -0.016), (0.0062, 0.0062), (-0.007, -0.007), (0.0032, 0.0032)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(27):\n",
    "  compare_lst =     [\n",
    "    (round(c1.item(),4), round(c2.item(),4))\n",
    "    for c1, c2 in zip(dC_it[i], dC_vec[i])\n",
    "    if not (c1==c2==0.)\n",
    "  ]\n",
    "  if compare_lst:\n",
    "    print(compare_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "for cix, grad in zip(Xb.view(-1), demb.view(-1,demb.shape[-1])):\n",
    "  dC[cix] += grad\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "mO-8aqxK8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "with torch.no_grad():\n",
    "  # -----------------\n",
    "  # YOUR CODE HERE :)\n",
    "  # -----------------\n",
    "  dlogprobs = - F.one_hot(Yb, vocab_size).float() * 1/n\n",
    "  cmp('logprobs', dlogprobs, logprobs)\n",
    "  dprobs = dlogprobs / probs.data\n",
    "  cmp('probs', dprobs, probs)\n",
    "  dcounts_sum_inv = (dprobs * counts.data).sum(dim=1, keepdims=True)\n",
    "  cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "  dcounts_sum = -dcounts_sum_inv * counts_sum**-2\n",
    "  cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "  dcounts = (\n",
    "    (dcounts_sum)  # (nx1) because all the elements of a row in counts are added, the gradient of the added row simply flows back (addn saves grad)\n",
    "    + (dprobs * counts_sum_inv) # (nxk) * (nx1)\n",
    "  )\n",
    "  cmp('counts', dcounts, counts)\n",
    "  dnorm_logits = dcounts * counts\n",
    "  cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "  dlogit_maxes = -dnorm_logits.sum(dim=1, keepdim=True)\n",
    "  cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "  dlogits = dnorm_logits.clone() # (probs.data - F.one_hot(Yb, vocab_size)) * 1/n\n",
    "  dlogits[range(n), logits.max(1).indices] += dlogit_maxes.squeeze()\n",
    "  cmp('logits', dlogits, logits)\n",
    "  dh = dlogits @ W2.T\n",
    "  cmp('h', dh, h)\n",
    "  dW2 = h.T @ dlogits\n",
    "  cmp('W2', dW2, W2)\n",
    "  db2 = dlogits.sum(dim=0)\n",
    "  cmp('b2', db2, b2)\n",
    "  dhpreact = dh * (1-h*h)\n",
    "  cmp('hpreact', dhpreact, hpreact)\n",
    "  dbngain =  (dhpreact * bnraw).sum(dim=0, keepdims=True)\n",
    "  cmp('bngain', dbngain, bngain)\n",
    "  dbnbias = dhpreact.sum(dim=0)\n",
    "  cmp('bnbias', dbnbias, bnbias)\n",
    "  dbnraw = dhpreact * bngain\n",
    "  cmp('bnraw', dbnraw, bnraw)\n",
    "  dbnvar_inv = (dbnraw * bndiff).sum(dim=0, keepdims=True)\n",
    "  cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "  dbnvar = dbnvar_inv * (-0.5) * (bnvar + 1e-5)**(-1.5)\n",
    "  cmp('bnvar', dbnvar, bnvar)\n",
    "  dbndiff2 = torch.ones_like(bndiff2) * dbnvar * (1/(n-1))\n",
    "  cmp('bndiff2', dbndiff2, bndiff2)\n",
    "  dbndiff = (\n",
    "    (2 * dbndiff2 * bndiff)\n",
    "    + dbnraw * bnvar_inv\n",
    "  )\n",
    "  cmp('bndiff', dbndiff, bndiff)\n",
    "  dbnmeani = - dbndiff.sum(axis=0, keepdims=True)\n",
    "  cmp('bnmeani', dbnmeani, bnmeani)\n",
    "  dhprebn = (\n",
    "    dbndiff\n",
    "    + (torch.ones_like(hprebn) * dbnmeani * (1/n))\n",
    "  )\n",
    "  cmp('hprebn', dhprebn, hprebn)\n",
    "  dembcat = dhprebn @ W1.T\n",
    "  cmp('embcat', dembcat, embcat)\n",
    "  dW1 = embcat.T @ dhprebn\n",
    "  cmp('W1', dW1, W1)\n",
    "  db1 = dhprebn.sum(axis=0)\n",
    "  cmp('b1', db1, b1)\n",
    "  demb = dembcat.view(emb.shape)\n",
    "  cmp('emb', demb, emb)\n",
    "  dC = torch.zeros_like(C)\n",
    "  for cix, grad in zip(Xb.view(-1), demb.view(-1,demb.shape[-1])):\n",
    "    dC[cix] += grad\n",
    "  cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "ebLtYji_8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4191508293151855 diff: -4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "id": "-gCXbB4C8PPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = (1/n) * (probs.data - F.one_hot(Yb, vocab_size))\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x17ed36140>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(dlogits, cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "id": "hd-MkhB68PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "id": "POdeZSKT8PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "id": "wPy8DhqB8PPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7900\n",
      "  10000/ 200000: 2.1867\n",
      "  20000/ 200000: 2.3885\n",
      "  30000/ 200000: 2.4745\n",
      "  40000/ 200000: 1.9757\n",
      "  50000/ 200000: 2.4217\n",
      "  60000/ 200000: 2.3510\n",
      "  70000/ 200000: 2.0539\n",
      "  80000/ 200000: 2.4223\n",
      "  90000/ 200000: 2.1433\n",
      " 100000/ 200000: 2.0089\n",
      " 110000/ 200000: 2.2877\n",
      " 120000/ 200000: 2.0273\n",
      " 130000/ 200000: 2.4873\n",
      " 140000/ 200000: 2.2530\n",
      " 150000/ 200000: 2.2041\n",
      " 160000/ 200000: 2.0072\n",
      " 170000/ 200000: 1.8101\n",
      " 180000/ 200000: 2.0145\n",
      " 190000/ 200000: 1.8702\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "  #   loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    # YOUR CODE HERE :)\n",
    "    dlogits = (F.softmax(logits,dim=1) - F.one_hot(Yb, vocab_size)) * 1/n\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(dim=0)\n",
    "    dhpreact = dh * (1-h*h)\n",
    "    dbngain =  (dhpreact * bnraw).sum(dim=0, keepdims=True)\n",
    "    dbnbias = dhpreact.sum(dim=0)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(axis=0)\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for cix, grad in zip(Xb.view(-1), demb.view(-1,demb.shape[-1])):\n",
    "      dC[cix] += grad\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "  #     p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "id": "ZEpI0hMW8PPz"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all(): argument 'input' (position 1) must be Tensor, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [373]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# useful for checking your gradients\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parameters, grads):\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mcmp\u001b[0;34m(s, dt, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcmp\u001b[39m(s, dt, t):\n\u001b[0;32m----> 3\u001b[0m   ex \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      4\u001b[0m   app \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(dt, t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m      5\u001b[0m   maxdiff \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: all(): argument 'input' (position 1) must be Tensor, not bool"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "  cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "id": "KImLWNoh8PP0"
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "id": "6aFnP_Zc8PP0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.069404125213623\n",
      "val 2.1058788299560547\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "id": "esWqmhyj8PP1"
   },
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "id": "xHeQNv3s8PP1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahza.\n",
      "jahmerik.\n",
      "mrix.\n",
      "taty.\n",
      "sacarlie.\n",
      "mahnen.\n",
      "delynn.\n",
      "jareei.\n",
      "nellara.\n",
      "chaiivon.\n",
      "leigh.\n",
      "ham.\n",
      "joce.\n",
      "quinn.\n",
      "shois.\n",
      "alianni.\n",
      "watell.\n",
      "dearyn.\n",
      "kai.\n",
      "eveigh.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
